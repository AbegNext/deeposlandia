#+TITLE: Detecting objects starting from street-scene images
#+AUTHOR: Raphaël Delhome <raphael.delhome@oslandia.com>

Exploiting artificial intelligence within the geospatial data context tends to
be easier and easier thanks to emerging deep learning techniques. Neural
networks take indeed various kinds of designs, and cope with a wide range of
applications.

At [[http://oslandia.com/en/home-en/][Oslandia]] we bet that these techniques will have an added-value in our daily
activity, as data is of first importance for us.

This article will show you an example of how to use AI techniques so as to
address typical use cases.

* Exploit an open dataset related to street-scene

In this article we use a set of 25,000 images provided by [[https://www.mapillary.com/][Mapillary]], in order to
investigate on the presence of some typical street-scene objects (vehicles,
roads, pedestrians...). Mapillary released this dataset recently, it is [[https://www.mapillary.com/dataset/vistas][still
available on its website]] and may be downloaded freely for a research purpose.

As inputs, Mapillary provides a bunch of street scene images of various sizes
in a =images= repository, and the same images after filtering process in
=instances= and =labels= repositories. The latter is crucial, as the filtered
images are actually composed of pixels in a reduced set of colors. Actually,
there is one color per object types; and 66 object types in total. Some minor
operations on the image pixels can give outputs as one-hot vectors (/i.e./ a
vector of =0= and =1=, =1= if the corresponding label is on the image, =0=
otherwise).

#+CAPTION: Example of image, with its filtered version
#+NAME:   fig:ex_mapillary_image
#+ATTR_HTML: width="30px"
[[../images/MVD_M2kh294N9c72sICO990Uew.png]]

As a remark, neural networks consider equally-sized inputs. A first
approximation could be to resize every image as the most encountered size
(=2448*3264=), however we choose to resize them at a smaller size (=576*768=)
for computation purpose.

* Implement a convolutional neural network with TensorFlow

Our goal here is to predict the presence of differents street-scene components
on pictures. We aim to train a neural network model to make it able to detect
if there is car(s), truck(s) or bicycle(s) for instance on images.

As Mapillary provided a set of 66 labels and a labelled version of each dataset
image, we plan to investigate a multilabel classification problem, where the
final network layer must evaluate if there is an occurrence of each label on
any image.

#+BEGIN_SRC ipython :session mapcnn :exports none
import tensorflow as tf
#+END_SRC

#+RESULTS:

** Neural network global structure

Handling image within neural network is generally done with the help of
*convolutional neural network*. They are composed of several kind of layers
that must be described:

+ some convolutional layers, in which images are filtered by several learnable
  image kernels, so as to extract image patterns based on pixels (this layer
  type is very important in convolutional neural network);
+ some pooling layers, in order to reduce the size of images and converge
  towards output layer, as well as to extract feature rough locations (the =max=
  pooling operation is the most common one, /i.e./ consider the maximal value
  over a local set of pixels);
+ some fully-connected layers, where every neurons of the current layer are
  connected to every neurons of the previous layer.

#+CAPTION: Convolutional neural network illustration (/cf/ Wikipedia)
#+NAME:   fig:cnn_illustration
#+ATTR_HTML: width="30px"
[[https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png]]

We've carried out a set of tests with different hyperparameter values, /i.e./
different amounts of each layer kinds. The results are globally stable if we
consider more than one convolutional layer. Here comes the way to define a
neural network with [[https://www.tensorflow.org/][TensorFlow]], the dedicated Python library.

# dag : ça fonctionne 'import sources.' même si y'a pas de =__init__.py= dans ton
# répertoire ?

#+BEGIN_SRC ipython :session mapcnn :exports none
import sources.cnn_layers as cnnl

train_image_batch, train_label_batch, train_filename_batch = \
cnnl.prepare_data(576, 768, 3, 20, "training", "training_data_pipe")
#+END_SRC

#+RESULTS:

** How to define data

Inputs and outputs are defined as "placeholders", /aka/ a sort of variables
that must be feed by data.

#+BEGIN_SRC ipython :session mapcnn :exports code
X = tf.placeholder(tf.float32, [None, 576, 768, 3], name='X')
Y = tf.placeholder(tf.float32, [None, 66], name='Y')
#+END_SRC

#+RESULTS:

** How to define a convolutional layer

After designing the kernel and the biases, we can use the TensorFlow function
=conv2d= to build this layer.

#+BEGIN_SRC ipython :session mapcnn :exports code
kernel1 = tf.get_variable('kernel1',
                         [8, 8, 3, 16],
                         initializer=tf.truncated_normal_initializer())
biases1 = tf.get_variable('biases1',
                         [16],
                         initializer=tf.constant_initializer(0.0))
# Apply the image convolution with a ReLu activation function
conv_layer1 = tf.nn.relu(tf.add(tf.nn.conv2d(X, kernel1, strides=[1, 1, 1, 1], padding="SAME"), biases1))
#+END_SRC

In this example, the kernel are 16 squares of 8*8 pixels considering 3 colors
(RGB channels).

#+RESULTS:

** How to define a max-pooling layer

As for convolutional layer, there is a ready-to-use function in the TensorFlow API,
/i.e./ =max_pool=.

#+BEGIN_SRC ipython :session mapcnn :exports code
pool_layer1 = tf.nn.max_pool(conv_layer1, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')
#+END_SRC

This function takes the maximal pixel value for each block of 4*4 pixels, in
every filtered image. The out-of-the-border pixels are set as the border
pixels, if a block definition needs such additional information. The number of
pixels is divided by 16 after such an operation.

#+RESULTS:

#+BEGIN_SRC ipython :session mapcnn :exports none
kernel2 = tf.get_variable('kernel2',
                         [8, 8, 16, 24],
                         initializer=tf.truncated_normal_initializer())
biases2 = tf.get_variable('biases2',
                         [24],
                         initializer=tf.constant_initializer(0.0))
# Apply the image convolution with a ReLu activation function
conv_layer2 = tf.nn.relu(tf.add(tf.nn.conv2d(pool_layer1, kernel2, strides=[1, 1, 1, 1], padding="SAME"), biases2))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session mapcnn :exports none
pool_layer2 = tf.nn.max_pool(conv_layer2, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')
#+END_SRC

#+RESULTS:

** How to define a fully-connected layer

This operation corresponds to a standard matrix multiplication; we just have to
reshape the output of the previous layer so as to consider comparable
structures. Let's imagine we add a second convolutional layer as well as second
max-pooling layer, the full-connected layer definition is as follows:

#+BEGIN_SRC ipython :session mapcnn :exports code
reshaped = tf.reshape(pool_layer2, [-1, int((576/(4*4))*(768/(4*4))*24)])
# Create weights and biases
weights_fc = tf.get_variable('weights_fullconn', [int((576/(4*4))*(768/(4*4))*24), 1024],
                    initializer=tf.truncated_normal_initializer())
biases_fc = tf.get_variable('biases_fullconn', [1024],
                    initializer=tf.constant_initializer(0.0))
# Apply relu on matmul of reshaped and w + b
fc = tf.nn.relu(tf.add(tf.matmul(reshaped, weights_fc), biases_fc), name='relu')
# Apply dropout
fc_layer = tf.nn.dropout(fc, 0.75, name='relu_with_dropout')
#+END_SRC

#+RESULTS:

Here we have defined the major part of our network. However the output layer is
still missing...

** Build predicted labels

The predicted labels are given after a sigmoid activation in the last layer:
even if other performing activation functions exist, the sigmoid function
allows to consider independant probabilities in multilabel context, /i.e./ the
presence of different object types on images is possible.

The sigmoid function gives probabilities of appearance of each object type, for
a given picture. The predicted labels are built as simply as possible: a
threshold of =0.5= is set to differentiate negative and positive predictions.

#+BEGIN_SRC ipython :session mapcnn :exports code
# Create weights and biases for the final fully-connected layer
weights_sig = tf.get_variable('weights_s', [1024, 66],
                    initializer=tf.truncated_normal_initializer())
biases_sig = tf.get_variable('biases_s', [66],
                    initializer=tf.random_normal_initializer())
logits = tf.add(tf.matmul(fc_layer, weights_sig), biases_sig)
Y_raw_predict = tf.nn.sigmoid(logits)
Y_predict = tf.to_int32(tf.round(Y_raw_predict))
#+END_SRC

#+RESULTS:

** Optimize the network

Although several metrics may measure the model convergence, we choose to
consider classic cross-entropy between true and predicted labels.

#+BEGIN_SRC ipython :session mapcnn :exports code
entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)
loss = tf.reduce_mean(entropy, name="loss")
optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)
#+END_SRC

#+RESULTS:

In this snippet, we are using =AdamOptimizer=, however other solutions do exist
(/e.g./ =GradientDescentOptimizer=).

** Assess the model quality

Several way of measuring the model quality may be computed, see /e.g./:

+ accuracy (number of good predictions, over total number of predictions)
+ precision (number of true positives over all positive predictions)
+ recall (number of true positives over all real positive values)

They can be computed globally, or by label, as we are in a multilabel
classification problem.

** Train the model

Last but not least, we have to train the model we have defined. That's a bit
complicated because of batching operations, for a sake of clarity here we
suppose that our training data are correctly batched and we loop over 100
iterations only, to keep the training short (that's just for demo, prefer
considering all your data *at least* once!).

#+BEGIN_SRC ipython :session mapcnn :exports code
from sklearn.metrics import accuracy_score

def unnest(l):
    return [index for sublist in l for index in sublist]

sess = tf.Session()
# Initialize the tensorflow variables
sess.run(tf.global_variables_initializer())
    
# Initialize threads to begin batching operations
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(coord=coord, sess=sess)
    
# Train the model
for index in range(20):
    X_batch, Y_batch = sess.run([train_image_batch, train_label_batch])
    sess.run(optimizer, feed_dict={X: X_batch, Y: Y_batch})
    if index % 5 == 0:
        Y_pred, loss_batch = sess.run([Y_predict, loss], feed_dict={X: X_batch, Y: Y_batch})
        accuracy_batch = accuracy_score(unnest(Y_batch), unnest(Y_pred))
        print("""Step {}: loss = {:5.3f}, accuracy={:1.3f}""".format(index, loss_batch, accuracy_batch))
    
# Stop the threads used during the process
coord.request_stop()
coord.join(threads)
#+END_SRC

#+RESULTS:

* What kind of objects are on a test image ?

In order to illstrate the previous sections, we can test our network on a new
image, /i.e./ an image that does not have been scanned during model training.

#+CAPTION: Example of image used to validate the model
#+NAME:   fig:ex_validation_image
#+ATTR_HTML: width="30px"
[[../data/validation/inputs/00001.jpg]]

#+BEGIN_SRC ipython :session mapcnn :exports none
from PIL import Image
import numpy as np
import pandas as pd

image = Image.open("../data/validation/input/00001.jpg")
x_test = np.array(image).reshape([1, 576, 768, 3])

labels = pd.read_csv("../data/validation/output/labels.csv")
y_test = labels.query("new_name=='00001.jpg'").iloc[:,6:].values
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session mapcnn :exports none
import json

with open('../data/config.json') as config_file:
    config = json.load(config_file)
config.keys()
label_description = config['labels']
labels = [l['name'] for l in label_description]
label_names = [l.split('--')[len(l.split('--'))-1] for l in labels]
label_names
#+END_SRC

#+RESULTS:
| bird | ground-animal | curb | fence | guard-rail | other-barrier | wall | bike-lane | crosswalk-plain | curb-cut | parking | pedestrian-area | rail-track | road | service-lane | sidewalk | bridge | building | tunnel | person | bicyclist | motorcyclist | other-rider | crosswalk-zebra | general | mountain | sand | sky | snow | terrain | vegetation | water | banner | bench | bike-rack | billboard | catch-basin | cctv-camera | fire-hydrant | junction-box | mailbox | manhole | phone-booth | pothole | street-light | pole | traffic-sign-frame | utility-pole | traffic-light | back | front | trash-can | bicycle | boat | bus | car | caravan | motorcycle | on-rails | other-vehicle | trailer | truck | wheeled-slow | car-mount | ego-vehicle | unlabeled |

The neural network is supplied with this image and the corresponding true
labels, to compute predicted labels:

#+BEGIN_SRC ipython :session mapcnn :exports code
Y_pred, loss_batch = sess.run([Y_predict, loss], feed_dict={X: x_test, Y: y_test})
sess.close()
#+END_SRC

#+BEGIN_SRC ipython :session mapcnn :exports none
y_test[0]
#+END_SRC:

#+RESULTS:

By comparing the output given by the model and the true output, we can assess
the model accuracy. In this case, we focus on the confusion matrix:

#+BEGIN_SRC ipython :session mapcnn :exports results
from sklearn.metrics import confusion_matrix

pd.DataFrame(confusion_matrix(y_test[0], Y_pred[0]), columns=["Y_pred=False", "Y_pred=True"], index=["y_test=False", "y_test=True"])
#+END_SRC

#+RESULTS:
:               Y_pred=False  Y_pred=True
: y_test=False            34            9
: y_test=True              8           15

The model accuracy for this image is around =74.2% ((34+15)/66)=, which is
quite good. However it may certainly be improved as the model scanned training
images only once...

We can extract the label that are maybe the more interesting category, /aka/
the /true positives/ corresponding to object on the image detected by the
model:

#+BEGIN_SRC ipython :session mapcnn :exports results
import itertools

true_positive = list(itertools.compress(label_names, np.logical_and(y_test[0], Y_pred[0])))
pd.Series(true_positive)
#+END_SRC

#+RESULTS:
#+begin_example
0              curb
1              road
2          sidewalk
3          building
4            person
5           general
6               sky
7        vegetation
8         billboard
9      street-light
10             pole
11     utility-pole
12    traffic-light
13            truck
14        unlabeled
dtype: object
#+end_example

To understand the category taxonomy, interested readers may read the [[http://research.mapillary.com/img/publications/ICCV17a.pdf][dedicated
paper]] available on Mapillary website.

* How to go further?

In this post we've just considered a feature detection problem, so as to decide
if an object type =t= is really on an image =p=, or not. The natural
prolongation of that is the semantic segmentation, /i.e./ knowing which
pixel(s) of =p= have to be labeled as part of an object of type =t=.

This is the way Mapillary labelled the pictures; it is without any doubt a
really promising research field for some use cases related to geospatial data!

To go deeper into this analysis, you can find our code on [[https://github.com/Oslandia/mapillary_classification][Github]].

If you want to collaborate with us and be a R&D partner, do not hesitate to
contact us at [[infos@oslandia.com]]!
