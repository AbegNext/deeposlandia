{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a neural network is complicated, however finding a convenient (or, *let be optimistic* optimal) set of hyperparameters is even harder.\n",
    "\n",
    "By *hyperparameter* we mean all the values that define the run. They can be related to the network structure, or to the training process:\n",
    "- number of layers, number of neurons (more generally, network structure)\n",
    "- batch size\n",
    "- image size\n",
    "- optimizer\n",
    "- learning rate (starting learning rate, decay step, decay rate)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the hyperparameter optimization in this notebook, we use `hyperas`, a [Python library](https://github.com/maxpumperla/hyperas) that wraps [hyperopt](https://github.com/hyperopt/hyperopt), another famous Python library dedicated to parameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to import the needed dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rde/.envs/cnn/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import STATUS_OK, tpe, Trials\n",
    "from hyperas import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other dependencies are related to our own code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeposlandia.generator import create_generator\n",
    "from deeposlandia.keras_feature_detection import FeatureDetectionNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    DATASET = \"shapes\"\n",
    "    MODEL = \"feature_detection\"\n",
    "    IMAGE_SIZE = 64\n",
    "    DATAPATH = os.path.join(\"..\", \"data\", DATASET, \"preprocessed\", str(IMAGE_SIZE) + \"_full\")\n",
    "    BATCH_SIZE = 50\n",
    "    LABELS = range(3)\n",
    "    SEED = 1337\n",
    "    train_generator = create_generator(dataset=DATASET,\n",
    "                                       model=MODEL,\n",
    "                                       datapath=os.path.join(DATAPATH, \"training\"), \n",
    "                                       image_size=IMAGE_SIZE,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       label_ids=LABELS,\n",
    "                                       seed=SEED)\n",
    "    validation_generator = create_generator(dataset=DATASET,\n",
    "                                            model=MODEL,\n",
    "                                            datapath=os.path.join(DATAPATH, \"validation\"),\n",
    "                                            image_size=IMAGE_SIZE,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            label_ids=LABELS,\n",
    "                                            seed=SEED)\n",
    "    testing_generator = create_generator(dataset=DATASET,\n",
    "                                         model=MODEL,\n",
    "                                         datapath=os.path.join(DATAPATH, \"testing\"),\n",
    "                                         image_size=IMAGE_SIZE,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         label_ids=LABELS,\n",
    "                                         seed=SEED,\n",
    "                                         inference=True)\n",
    "    return train_generator, validation_generator, testing_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_unbatch():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    DATASET = \"shapes\"\n",
    "    MODEL = \"feature_detection\"\n",
    "    IMAGE_SIZE = 64\n",
    "    DATAPATH = os.path.join(\"..\", \"data\", DATASET, \"preprocessed\", str(IMAGE_SIZE) + \"_full\")\n",
    "    BATCH_SIZE = 50\n",
    "    LABELS = range(3)\n",
    "    SEED = 1337\n",
    "    train_generator = create_generator(dataset=DATASET,\n",
    "                                       model=MODEL,\n",
    "                                       datapath=os.path.join(DATAPATH, \"training\"), \n",
    "                                       image_size=IMAGE_SIZE,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       label_ids=LABELS,\n",
    "                                       seed=SEED)\n",
    "    validation_generator = create_generator(dataset=DATASET,\n",
    "                                            model=MODEL,\n",
    "                                            datapath=os.path.join(DATAPATH, \"validation\"),\n",
    "                                            image_size=IMAGE_SIZE,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            label_ids=LABELS,\n",
    "                                            seed=SEED)\n",
    "    testing_generator = create_generator(dataset=DATASET,\n",
    "                                         model=MODEL,\n",
    "                                         datapath=os.path.join(DATAPATH, \"testing\"),\n",
    "                                         image_size=IMAGE_SIZE,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         label_ids=LABELS,\n",
    "                                         seed=SEED)\n",
    "    train_X, train_Y = next(train_generator)\n",
    "    val_X, val_Y = next(validation_generator)\n",
    "    test_X = next(testing_generator)\n",
    "    return train_X, train_Y, val_X, val_Y, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_generator, validation_generator):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    IMAGE_SIZE = 64\n",
    "    BATCH_SIZE = 50\n",
    "    NB_EPOCHS = 10\n",
    "    NB_TRAINING_IMAGES = 1000\n",
    "    NB_VALIDATION_IMAGES = 200\n",
    "    cnn = FeatureDetectionNetwork(\"test\", image_size=IMAGE_SIZE, nb_labels=len(LABELS))\n",
    "    model = Model(cnn.X, cnn.Y)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer={{choice(['adam', 'sgd', 'adagrad'])}},\n",
    "                  metrics=['acc'])\n",
    "    model.fit_generator(train_generator,\n",
    "                        epochs=NB_EPOCHS,\n",
    "                        steps_per_epoch=NB_TRAINING_IMAGES // BATCH_SIZE)\n",
    "    score, acc = model.evaluate_generator(validation_generator, steps=NB_VALIDATION_IMAGES // BATCH_SIZE)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_unbatch(train_X, train_Y, val_X, val_Y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    IMAGE_SIZE = 64\n",
    "    BATCH_SIZE = 50\n",
    "    NB_EPOCHS = 10\n",
    "    NB_TRAINING_IMAGES = 1000\n",
    "    NB_VALIDATION_IMAGES = 200\n",
    "    cnn = FeatureDetectionNetwork(\"test\", image_size=IMAGE_SIZE, nb_labels=len(LABELS))\n",
    "    model = Model(cnn.X, cnn.Y)\n",
    "    adam = Adam(lr={{choice(0.01, 0.001, 0.0001)}}, decay={{choice(1e-3, 1e-4, 1e-5)}})\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optim,\n",
    "                  metrics=['acc'])\n",
    "    model.fit(x=train_X, y=train_Y, epochs=NB_EPOCHS,\n",
    "                        steps_per_epoch=NB_TRAINING_IMAGES // BATCH_SIZE)\n",
    "    score, acc = model.predict(x=val_X ,y=val_Y, steps=NB_VALIDATION_IMAGES // BATCH_SIZE)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 images belonging to 1 classes.\n",
      "Found 18000 images belonging to 1 classes.\n",
      "Found 200 images belonging to 1 classes.\n",
      "Found 200 images belonging to 1 classes.\n",
      "Found 5000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator, test_generator = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from hyperopt import STATUS_OK, tpe, Trials\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from deeposlandia.generator import create_generator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from deeposlandia.keras_feature_detection import FeatureDetectionNetwork\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'sgd', 'adagrad']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\"\n",
      "   3: \"\"\"\n",
      "   4: DATASET = \"shapes\"\n",
      "   5: MODEL = \"feature_detection\"\n",
      "   6: IMAGE_SIZE = 64\n",
      "   7: DATAPATH = os.path.join(\"..\", \"data\", DATASET, \"preprocessed\", str(IMAGE_SIZE) + \"_full\")\n",
      "   8: BATCH_SIZE = 50\n",
      "   9: LABELS = range(3)\n",
      "  10: SEED = 1337\n",
      "  11: train_generator = create_generator(dataset=DATASET,\n",
      "  12:                                    model=MODEL,\n",
      "  13:                                    datapath=os.path.join(DATAPATH, \"training\"), \n",
      "  14:                                    image_size=IMAGE_SIZE,\n",
      "  15:                                    batch_size=BATCH_SIZE,\n",
      "  16:                                    label_ids=LABELS,\n",
      "  17:                                    seed=SEED)\n",
      "  18: validation_generator = create_generator(dataset=DATASET,\n",
      "  19:                                         model=MODEL,\n",
      "  20:                                         datapath=os.path.join(DATAPATH, \"validation\"),\n",
      "  21:                                         image_size=IMAGE_SIZE,\n",
      "  22:                                         batch_size=BATCH_SIZE,\n",
      "  23:                                         label_ids=LABELS,\n",
      "  24:                                         seed=SEED)\n",
      "  25: testing_generator = create_generator(dataset=DATASET,\n",
      "  26:                                      model=MODEL,\n",
      "  27:                                      datapath=os.path.join(DATAPATH, \"testing\"),\n",
      "  28:                                      image_size=IMAGE_SIZE,\n",
      "  29:                                      batch_size=BATCH_SIZE,\n",
      "  30:                                      label_ids=LABELS,\n",
      "  31:                                      seed=SEED,\n",
      "  32:                                      inference=False)\n",
      "  33: \n",
      "  34: \n",
      "  35: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     \"\"\"\n",
      "  4:     \"\"\"\n",
      "  5:     IMAGE_SIZE = 64\n",
      "  6:     BATCH_SIZE = 50\n",
      "  7:     NB_EPOCHS = 10\n",
      "  8:     NB_TRAINING_IMAGES = 1000\n",
      "  9:     NB_VALIDATION_IMAGES = 200\n",
      " 10:     cnn = FeatureDetectionNetwork(\"test\", image_size=IMAGE_SIZE, nb_labels=len(LABELS))\n",
      " 11:     model = Model(cnn.X, cnn.Y)\n",
      " 12:     model.compile(loss='binary_crossentropy',\n",
      " 13:                   optimizer=space['optimizer'],\n",
      " 14:                   metrics=['acc'])\n",
      " 15:     model.fit_generator(train_generator,\n",
      " 16:                         epochs=NB_EPOCHS,\n",
      " 17:                         steps_per_epoch=NB_TRAINING_IMAGES // BATCH_SIZE)\n",
      " 18:     score, acc = model.evaluate_generator(validation_generator, steps=NB_VALIDATION_IMAGES // BATCH_SIZE)\n",
      " 19:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      " 20: \n",
      "Found 18000 images belonging to 1 classes.\n",
      "Found 18000 images belonging to 1 classes.\n",
      "Found 200 images belonging to 1 classes.\n",
      "Found 200 images belonging to 1 classes.\n",
      "Found 5000 images belonging to 1 classes.\n",
      "Found 0 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 7s 328ms/step - loss: 1.0129 - acc: 0.5193\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8889 - acc: 0.5227\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.8015 - acc: 0.5753\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 6s 310ms/step - loss: 0.7626 - acc: 0.5960\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.7420 - acc: 0.6173\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.7207 - acc: 0.6233\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.6913 - acc: 0.6447\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.6984 - acc: 0.6450\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.6801 - acc: 0.6453\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.6560 - acc: 0.6643\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 7s 338ms/step - loss: 0.9140 - acc: 0.5887\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 6s 310ms/step - loss: 0.8450 - acc: 0.6167\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 6s 310ms/step - loss: 0.8343 - acc: 0.6127\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 6s 311ms/step - loss: 0.7755 - acc: 0.6217\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 6s 310ms/step - loss: 0.7738 - acc: 0.6247\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 6s 311ms/step - loss: 0.7439 - acc: 0.6310\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.7139 - acc: 0.6507\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 6s 311ms/step - loss: 0.7030 - acc: 0.6463\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 6s 311ms/step - loss: 0.7195 - acc: 0.6267\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.6767 - acc: 0.6607\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 7s 331ms/step - loss: 0.9626 - acc: 0.5437\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.8995 - acc: 0.5523\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8505 - acc: 0.6033\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.8689 - acc: 0.5837\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8352 - acc: 0.5943\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.8414 - acc: 0.6017\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8260 - acc: 0.5960\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.8154 - acc: 0.6023\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.7920 - acc: 0.6227\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8073 - acc: 0.6183\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 7s 331ms/step - loss: 0.9526 - acc: 0.5270\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.9119 - acc: 0.5560\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8848 - acc: 0.5650\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8619 - acc: 0.5787\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.8569 - acc: 0.5880\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.8386 - acc: 0.5907\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 6s 308ms/step - loss: 0.8490 - acc: 0.5890\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8101 - acc: 0.6053\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 6s 306ms/step - loss: 0.7873 - acc: 0.6213\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 6s 307ms/step - loss: 0.8069 - acc: 0.6053\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 7s 341ms/step - loss: 0.9241 - acc: 0.5837\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.8609 - acc: 0.6000\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.7925 - acc: 0.6180\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.7767 - acc: 0.6230\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 6s 309ms/step - loss: 0.7448 - acc: 0.6240\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 6s 311ms/step - loss: 0.7185 - acc: 0.6370\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 6s 311ms/step - loss: 0.7388 - acc: 0.6393\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 6s 312ms/step - loss: 0.7038 - acc: 0.6317\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 6s 312ms/step - loss: 0.7200 - acc: 0.6390\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 6s 311ms/step - loss: 0.6742 - acc: 0.6717\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='hyperparam_optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44647166, 0.42901668, 0.30491966],\n",
       "       [0.58880764, 0.83338577, 0.94683397],\n",
       "       [0.67029434, 0.577556  , 0.49348247],\n",
       "       ...,\n",
       "       [0.43205374, 0.51706254, 0.62923896],\n",
       "       [0.80425006, 0.68572044, 0.7020138 ],\n",
       "       [0.59167844, 0.5832041 , 0.61971694]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_TESTING_IMAGES = 5000\n",
    "BATCH_SIZE = 50\n",
    "best_model.predict_generator(test_generator, steps=NB_TESTING_IMAGES // BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
